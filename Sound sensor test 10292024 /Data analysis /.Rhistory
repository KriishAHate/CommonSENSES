geom_smooth(method = "lm", color = "red", se = FALSE) +  # Linear regression line
labs(
title = "Noise Averages: Small Hole vs Big Hole",  # Title of the plot
x = "Small Hole Noise Average dB",                 # Label for x-axis
y = "Big Hole Noise Average dB"                    # Label for y-axis
) +
theme_minimal() +  # Clean theme
# Add slope, intercept, R², and correlation coefficient as text annotation
annotate("text",
x = max(merged_data$Noise_avg_small_hole) * 0.8,
y = max(merged_data$Noise_avg_big_hole) * 0.9,
label = paste("Slope:", round(slope, 2),
"\nIntercept:", round(intercept, 2),
"\nR²:", round(r_squared, 2),
"\nCorrelation:", round(correlation, 2)),  # Added correlation to the label
color = "black", size = 5, hjust = 0)  # Position and style of the text
# Create the plot
ggplot(data = merged_data, aes(x = Noise_avg_small_hole, y = Noise_avg_big_hole)) +
geom_point(color = "blue", size = 2) +  # Scatter points
geom_smooth(method = "lm", color = "red", se = FALSE) +  # Linear regression line
labs(
title = "Noise Averages: Small Hole vs Big Hole",  # Title of the plot
x = "Small Hole Noise Average dB",                 # Label for x-axis
y = "Big Hole Noise Average dB"                    # Label for y-axis
) +
theme_minimal() +  # Clean theme
# Add slope, intercept, R², and correlation coefficient as text annotation
annotate("text",
x = max(merged_data$Noise_avg_small_hole) * 0.8,
y = max(merged_data$Noise_avg_big_hole) * 0.9,
label = paste("Slope:", round(slope, 2),
"\nIntercept:", round(intercept, 2),
"\nR²:", round(r_squared, 2),
"\nCorrelation:", round(correlation, 2)),  # Added correlation to the label
color = "black", size = 5, hjust = 0)  # Position and style of the text
```{r small hole vs big hole, include=FALSE}
# plot 3 small hole vs big hole
library(ggplot2)
# Load dplyr to use the pipe operator (%>%)
library(dplyr)
# Process result_small_hole datafram
# Remove rows with NA in the Noise_avg column
result_small_hole = result_small_hole %>% filter(!is.na(Noise_avg))
# Process result_big_hole dataframe
# Remove rows with NA in the Noise_avg column
result_big_hole = result_big_hole %>% filter(!is.na(Noise_avg))
# Merge the two dataframes on the 'BinMidpoint' column
merged_data = merge(
result_small_hole,       # First dataframe
result_big_hole,         # Second dataframe
by = "BinMidpoint",      # Column to merge on
suffixes = c("_small_hole", "_big_hole")  # Add suffixes to distinguish similar column names
)
# Fit a linear model to get the slope, intercept, and R-squared value
lm_model = lm(Noise_avg_big_hole ~ Noise_avg_small_hole, data = merged_data)
# Extract the slope (second coefficient), intercept (first coefficient), and R-squared value, and the Pearson correlation coefficient
slope = coef(lm_model)[2]  # The slope corresponds to Noise_avg_small_hole
intercept = coef(lm_model)[1]  # The intercept is the first coefficient
r_squared = summary(lm_model)$r.squared  # R-squared value
# Calculate the Pearson correlation coefficient
correlation = cor(merged_data$Noise_avg_small_hole, merged_data$Noise_avg_big_hole)
# Create the plot
ggplot(data = merged_data, aes(x = Noise_avg_small_hole, y = Noise_avg_big_hole)) +
geom_point(color = "blue", size = 2) +  # Scatter points
geom_smooth(method = "lm", color = "red", se = FALSE) +  # Linear regression line
labs(
title = "Noise Averages: Small Hole vs Big Hole",  # Title of the plot
x = "Small Hole Noise Average dB",                 # Label for x-axis
y = "Big Hole Noise Average dB"                    # Label for y-axis
) +
theme_minimal() +  # Clean theme
# Add slope, intercept, R², and correlation coefficient as text annotation
annotate("text",
x = max(merged_data$Noise_avg_small_hole) * 0.8,
y = max(merged_data$Noise_avg_big_hole) * 0.9,
label = paste("Slope:", round(slope, 2),
"\nIntercept:", round(intercept, 2),
"\nR²:", round(r_squared, 2),
"\nCorrelation:", round(correlation, 2)),  # Added correlation to the label
color = "black", size = 5, hjust = 0)  # Position and style of the text
# Create the plot
ggplot(data = merged_data, aes(x = Noise_avg_small_hole, y = Noise_avg_big_hole)) +
geom_point(color = "blue", size = 2) +  # Scatter points
geom_smooth(method = "lm", color = "red", se = FALSE) +  # Linear regression line
labs(
title = "Noise Averages: Small Hole vs Big Hole",  # Title of the plot
x = "Small Hole Noise Average dB",                 # Label for x-axis
y = "Big Hole Noise Average dB"                    # Label for y-axis
) +
theme_minimal() +  # Clean theme
# Add slope, intercept, R², and correlation coefficient as text annotation
annotate("text",
x = max(merged_data$Noise_avg_small_hole) * 0.8,
y = max(merged_data$Noise_avg_big_hole) * 0.9,
label = paste("Slope:", round(slope, 2),
"\nIntercept:", round(intercept, 2),
"\nR²:", round(r_squared, 2),
"\nCorrelation:", round(correlation, 2)),  # Added correlation to the label
color = "black", size = 5, hjust = 0)  # Position and style of the text
# Create the plot
ggplot(data = merged_data, aes(x = Noise_avg_open, y = Noise_avg_big_hole)) +
geom_point(color = "blue", size = 2) +  # Scatter points
geom_smooth(method = "lm", color = "red", se = FALSE) +  # Linear regression line
labs(
title = "Noise Averages: Open vs Big Hole",  # Title of the plot
x = "Open Noise Average dB",                 # Label for x-axis
y = "Big Hole Noise Average dB"              # Label for y-axis
) +
theme_minimal() +  # Clean theme
# Add slope, intercept, R², and correlation coefficient as text annotation
annotate("text",
x = max(merged_data$Noise_avg_open) * 0.8,
y = max(merged_data$Noise_avg_big_hole) * 0.9,
label = paste("Slope:", round(slope, 2),
"\nIntercept:", round(intercept, 2),
"\nR²:", round(r_squared, 2),
"\nCorrelation:", round(correlation, 2)),  # Added correlation to the label
color = "black", size = 5, hjust = 0)  # Position and style of the text
```{r open vs big hole, include=FALSE}
# plot 1 open vs big hole
library(ggplot2)
# Load dplyr to use the pipe operator (%>%)
library(dplyr)
# Process result_big_hole dataframe
# Remove rows with NA in the Noise_avg column
result_big_hole = result_big_hole %>% filter(!is.na(Noise_avg))
# Remove rows with NA in the Noise_avg column
result_open = result_open %>% filter(!is.na(Noise_avg))
# Merge the two dataframes on the 'BinMidpoint' column
# If BinMidpoint values do not match between the two dataframes, those rows will not appear in the merged dataset (this is known as an inner join, the default behavior in merge()).
# As long as the character strings in both dataframes have the same format (e.g., "YYYY-MM-DD HH:MM:SS"), the merge() function will correctly join the data based on the exact match of those strings.
merged_data = merge(
result_big_hole,       # First dataframe
result_open,           # Second dataframe
by = "BinMidpoint",    # Column to merge on
suffixes = c("_big_hole", "_open")  # Add suffixes to distinguish similar column names
)
# Fit a linear model to get the slope, intercept, and R-squared value
lm_model = lm(Noise_avg_big_hole ~ Noise_avg_open, data = merged_data)
# Extract the slope (coefficient), intercept (first coefficient), and R-squared value
slope = coef(lm_model)[2]  # The slope corresponds to Noise_avg_open
intercept = coef(lm_model)[1]  # The intercept is the first coefficient
r_squared = summary(lm_model)$r.squared  # R-squared value
# Calculate the Pearson correlation coefficient
correlation = cor(merged_data$Noise_avg_open, merged_data$Noise_avg_big_hole)
# Create the plot
ggplot(data = merged_data, aes(x = Noise_avg_open, y = Noise_avg_big_hole)) +
geom_point(color = "blue", size = 2) +  # Scatter points
geom_smooth(method = "lm", color = "red", se = FALSE) +  # Linear regression line
labs(
title = "Noise Averages: Open vs Big Hole",  # Title of the plot
x = "Open Noise Average dB",                 # Label for x-axis
y = "Big Hole Noise Average dB"              # Label for y-axis
) +
theme_minimal() +  # Clean theme
# Add slope, intercept, R², and correlation coefficient as text annotation
annotate("text",
x = max(merged_data$Noise_avg_open) * 0.8,
y = max(merged_data$Noise_avg_big_hole) * 0.9,
label = paste("Slope:", round(slope, 2),
"\nIntercept:", round(intercept, 2),
"\nR²:", round(r_squared, 2),
"\nCorrelation:", round(correlation, 2)),  # Added correlation to the label
color = "black", size = 5, hjust = 0)  # Position and style of the text
```{r open vs small hole, include=FALSE}
# plot 2 open vs small hole
library(ggplot2)
# Load dplyr to use the pipe operator (%>%)
library(dplyr)
# Process result_small_hole dataframe
# Remove rows with NA in the Noise_avg column
result_small_hole = result_small_hole %>% filter(!is.na(Noise_avg))
# Process result_open dataframe
# Remove rows with NA in the Noise_avg column
result_open = result_open %>% filter(!is.na(Noise_avg))
# Merge the two dataframes on the 'BinMidpoint' column
# If BinMidpoint values do not match between the two dataframes, those rows will not appear in the merged dataset (this is known as an inner join, the default behavior in merge()).
# As long as the character strings in both dataframes have the same format (e.g., "YYYY-MM-DD HH:MM:SS"), the merge() function will correctly join the data based on the exact match of those strings.
merged_data = merge(
result_small_hole,       # First dataframe
result_open,             # Second dataframe
by = "BinMidpoint",      # Column to merge on
suffixes = c("_small_hole", "_open")  # Add suffixes to distinguish similar column names
)
# Fit a linear model to get the slope, intercept, and R-squared value
lm_model = lm(Noise_avg_small_hole ~ Noise_avg_open, data = merged_data)
# Extract the slope (second coefficient), intercept (first coefficient), and R-squared value
slope = coef(lm_model)[2]  # The slope corresponds to Noise_avg_open
intercept = coef(lm_model)[1]  # The intercept is the first coefficient
r_squared = summary(lm_model)$r.squared  # R-squared value
# Calculate the Pearson correlation coefficient
correlation = cor(merged_data$Noise_avg_small_hole, merged_data$Noise_avg_open)
# Create the plot
ggplot(data = merged_data, aes(x = Noise_avg_open, y = Noise_avg_small_hole)) +
geom_point(color = "blue", size = 2) +  # Scatter points
geom_smooth(method = "lm", color = "red", se = FALSE) +  # Linear regression line
labs(
title = "Noise Averages: Open vs Small Hole",  # Title of the plot
x = "Open Noise Average dB",                   # Label for x-axis
y = "Small Hole Noise Average dB"              # Label for y-axis
) +
theme_minimal() +  # Clean theme
# Add slope, intercept, R², and correlation coefficient as text annotation
annotate("text",
x = max(merged_data$Noise_avg_open) * 0.8,
y = max(merged_data$Noise_avg_small_hole) * 0.9,
label = paste("Slope:", round(slope, 2),
"\nIntercept:", round(intercept, 2),  # Added intercept to the label
"\nR²:", round(r_squared, 2),
"\nCorrelation:", round(correlation, 2)),  # Added correlation to the label
color = "black", size = 5, hjust = 0)  # Position and style of the text
```{r small hole vs big hole, include=FALSE}
# plot 3 small hole vs big hole
library(ggplot2)
# Load dplyr to use the pipe operator (%>%)
library(dplyr)
# Process result_small_hole datafram
# Remove rows with NA in the Noise_avg column
result_small_hole = result_small_hole %>% filter(!is.na(Noise_avg))
# Process result_big_hole dataframe
# Remove rows with NA in the Noise_avg column
result_big_hole = result_big_hole %>% filter(!is.na(Noise_avg))
# Merge the two dataframes on the 'BinMidpoint' column
merged_data = merge(
result_small_hole,       # First dataframe
result_big_hole,         # Second dataframe
by = "BinMidpoint",      # Column to merge on
suffixes = c("_small_hole", "_big_hole")  # Add suffixes to distinguish similar column names
)
# Fit a linear model to get the slope, intercept, and R-squared value
lm_model = lm(Noise_avg_big_hole ~ Noise_avg_small_hole, data = merged_data)
# Extract the slope (second coefficient), intercept (first coefficient), and R-squared value, and the Pearson correlation coefficient
slope = coef(lm_model)[2]  # The slope corresponds to Noise_avg_small_hole
intercept = coef(lm_model)[1]  # The intercept is the first coefficient
r_squared = summary(lm_model)$r.squared  # R-squared value
# Calculate the Pearson correlation coefficient
correlation = cor(merged_data$Noise_avg_small_hole, merged_data$Noise_avg_big_hole)
# Create the plot
ggplot(data = merged_data, aes(x = Noise_avg_small_hole, y = Noise_avg_big_hole)) +
geom_point(color = "blue", size = 2) +  # Scatter points
geom_smooth(method = "lm", color = "red", se = FALSE) +  # Linear regression line
labs(
title = "Noise Averages: Small Hole vs Big Hole",  # Title of the plot
x = "Small Hole Noise Average dB",                 # Label for x-axis
y = "Big Hole Noise Average dB"                    # Label for y-axis
) +
theme_minimal() +  # Clean theme
# Add slope, intercept, R², and correlation coefficient as text annotation
annotate("text",
x = max(merged_data$Noise_avg_small_hole) * 0.8,
y = max(merged_data$Noise_avg_big_hole) * 0.9,
label = paste("Slope:", round(slope, 2),
"\nIntercept:", round(intercept, 2),
"\nR²:", round(r_squared, 2),
"\nCorrelation:", round(correlation, 2)),  # Added correlation to the label
color = "black", size = 5, hjust = 0)  # Position and style of the text
# Create the plot
ggplot(data = merged_data, aes(x = Noise_avg_small_hole, y = Noise_avg_big_hole)) +
geom_point(color = "blue", size = 2) +  # Scatter points
geom_smooth(method = "lm", color = "red", se = FALSE) +  # Linear regression line
labs(
title = "Noise Averages: Small Hole vs Big Hole",  # Title of the plot
x = "Small Hole Noise Average dB",                 # Label for x-axis
y = "Big Hole Noise Average dB"                    # Label for y-axis
) +
theme_minimal() +  # Clean theme
# Add slope, intercept, R², and correlation coefficient as text annotation
annotate("text",
x = max(merged_data$Noise_avg_small_hole) * 0.8,
y = max(merged_data$Noise_avg_big_hole) * 0.9,
label = paste("Slope:", round(slope, 2),
"\nIntercept:", round(intercept, 2),
"\nR²:", round(r_squared, 2),
"\nCorrelation:", round(correlation, 2)),  # Added correlation to the label
color = "black", size = 5, hjust = 0)  # Position and style of the text
knitr::opts_chunk$set(echo = TRUE)
# Load the dataset from the specified file path
big_hole_data = read.csv('/Users/kriishhate/Desktop/Dr. Mueller Lab Work/Sound sensor test 29:10:2024 /Processed files (start and end duration trimmed manually)/20241029_big_hole.csv')
# Combine the date and time columns into a single column named 'Datetime'
# Convert the combined column into a POSIXct datetime format for time-based operations
big_hole_data$Datetime = as.POSIXct(paste(big_hole_data$YYYYMMDD, big_hole_data$HHMMSS), format = "%Y%m%d %H%M%S")
# Define the size of each time interval in seconds
interval = 5
# Find the earliest timestamp in the dataset and assign it to 'start_time'
start_time = min(big_hole_data$Datetime)
# Find the latest timestamp in the dataset and assign it to 'end_time'
end_time = max(big_hole_data$Datetime)
# Generate a proper continuous time scale with regular intervals from 'start_time' to 'end_time'
# The sequence ensures no gaps in the time scale
proper_time_scale = seq(from = start_time, to = end_time, by = interval)
# Create an empty list to store the start times of each bin (interval)
time_bins = list()
# Create an empty list to store the midpoints of each bin (for plotting and correlation purposes)
bin_midpoints = list()
# Create an empty list to store the average noise values for each bin
noise_averages = list()
# Create an empty list to store the count of data points in each bin
bin_counts = list()
# Initialize the index to track the bin being processed and store results in the lists
i = 1
# Begin a loop that iterates through the proper time scale, excluding the last value
# The last value is excluded because it serves as the end time of the final bin
for (bin_start in proper_time_scale[-length(proper_time_scale)]) {
# Define the end time of the current bin by adding the interval size to 'bin_start'
bin_end = bin_start + interval
# Calculate the midpoint of the current bin as the average of 'bin_start' and 'bin_end'
bin_mid = bin_start + (bin_end - bin_start) / 2
# Filter the dataset to include only rows with timestamps within the current bin
bin_data = big_hole_data[big_hole_data$Datetime >= bin_start & big_hole_data$Datetime < bin_end, ]
# Count the number of rows in the filtered dataset to determine how many data points are in the bin
count = nrow(bin_data)
# Check if the current bin contains data points
if (count > 0) {
# If data points exist, calculate the average noise value for the bin
noise_avg = sum(bin_data$Noise) / count
} else {
# If no data points exist, assign NA as the average noise value for the bin
noise_avg = NA
}
# Add the start time of the current bin to the 'time_bins' list as a string
time_bins[[i]] = as.character(bin_start)
# Add the midpoint time of the current bin to the 'bin_midpoints' list as a string
bin_midpoints[[i]] = as.character(bin_mid)
# Add the calculated average noise value to the 'noise_averages' list
noise_averages[[i]] = noise_avg
# Add the count of data points in the bin to the 'bin_counts' list
bin_counts[[i]] = count
# Increment the index to track the next bin
i = i + 1
}
# Combine the lists into a data frame for easy analysis and visualization
result_big_hole = data.frame(
TimeBin = unlist(time_bins),        # Convert the 'time_bins' list into a column
BinMidpoint = unlist(bin_midpoints),# Convert the 'bin_midpoints' list into a column
Noise_avg = unlist(noise_averages), # Convert the 'noise_averages' list into a column
Count = unlist(bin_counts)          # Convert the 'bin_counts' list into a column
)
# Print the resulting data frame to view the processed bins and their statistics
print(result_big_hole)
knitr::opts_chunk$set(echo = TRUE)
# Load the dataset from the specified file path
small_hole_data = read.csv('/Users/kriishhate/Desktop/Dr. Mueller Lab Work/Sound sensor test 29:10:2024 /Processed files (start and end duration trimmed manually)/20241029_small_hole.csv')
# Combine the date and time columns into a single column named 'Datetime'
# Convert the combined column into a POSIXct datetime format for time-based operations
small_hole_data$Datetime = as.POSIXct(paste(small_hole_data$YYYYMMDD, small_hole_data$HHMMSS), format = "%Y%m%d %H%M%S")
# Define the size of each time interval in seconds
interval = 5
# Find the earliest timestamp in the dataset and assign it to 'start_time'
start_time = min(small_hole_data$Datetime)
# Find the latest timestamp in the dataset and assign it to 'end_time'
end_time = max(small_hole_data$Datetime)
# Generate a proper continuous time scale with regular intervals from 'start_time' to 'end_time'
# The sequence ensures no gaps in the time scale
proper_time_scale = seq(from = start_time, to = end_time, by = interval)
# Create an empty list to store the start times of each bin (interval)
time_bins = list()
# Create an empty list to store the midpoints of each bin (for plotting and correlation purposes)
bin_midpoints = list()
# Create an empty list to store the average noise values for each bin
noise_averages = list()
# Create an empty list to store the count of data points in each bin
bin_counts = list()
# Initialize the index to track the bin being processed and store results in the lists
i = 1
# Begin a loop that iterates through the proper time scale, excluding the last value
# The last value is excluded because it serves as the end time of the final bin
for (bin_start in proper_time_scale[-length(proper_time_scale)]) {
# Define the end time of the current bin by adding the interval size to 'bin_start'
bin_end = bin_start + interval
# Calculate the midpoint of the current bin as the average of 'bin_start' and 'bin_end'
bin_mid = bin_start + (bin_end - bin_start) / 2
# Filter the dataset to include only rows with timestamps within the current bin
bin_data = small_hole_data[small_hole_data$Datetime >= bin_start & small_hole_data$Datetime < bin_end, ]
# Count the number of rows in the filtered dataset to determine how many data points are in the bin
count = nrow(bin_data)
# Check if the current bin contains data points
if (count > 0) {
# If data points exist, calculate the average noise value for the bin
noise_avg = sum(bin_data$Noise) / count
} else {
# If no data points exist, assign NA as the average noise value for the bin
noise_avg = NA
}
# Add the start time of the current bin to the 'time_bins' list as a string
time_bins[[i]] = as.character(bin_start)
# Add the midpoint time of the current bin to the 'bin_midpoints' list as a string
bin_midpoints[[i]] = as.character(bin_mid)
# Add the calculated average noise value to the 'noise_averages' list
noise_averages[[i]] = noise_avg
# Add the count of data points in the bin to the 'bin_counts' list
bin_counts[[i]] = count
# Increment the index to track the next bin
i = i + 1
}
# Combine the lists into a data frame for easy analysis and visualization
result_small_hole = data.frame(
TimeBin = unlist(time_bins),        # Convert the 'time_bins' list into a column
BinMidpoint = unlist(bin_midpoints),# Convert the 'bin_midpoints' list into a column
Noise_avg = unlist(noise_averages), # Convert the 'noise_averages' list into a column
Count = unlist(bin_counts)          # Convert the 'bin_counts' list into a column
)
# Print the resulting data frame to view the processed bins and their statistics
print(result_small_hole)
knitr::opts_chunk$set(echo = TRUE)
# Load the dataset from the specified file path
open_data = read.csv('/Users/kriishhate/Desktop/Dr. Mueller Lab Work/Sound sensor test 29:10:2024 /Processed files (start and end duration trimmed manually)/20241029_open.csv')
# Combine the date and time columns into a single column named 'Datetime'
# Convert the combined column into a POSIXct datetime format for time-based operations
open_data$Datetime = as.POSIXct(paste(open_data$YYYYMMDD, open_data$HHMMSS), format = "%Y%m%d %H%M%S")
# Define the size of each time interval in seconds
interval = 5
# Find the earliest timestamp in the dataset and assign it to 'start_time'
start_time = min(open_data$Datetime)
# Find the latest timestamp in the dataset and assign it to 'end_time'
end_time = max(open_data$Datetime)
# Generate a proper continuous time scale with regular intervals from 'start_time' to 'end_time'
# The sequence ensures no gaps in the time scale
proper_time_scale = seq(from = start_time, to = end_time, by = interval)
# Create an empty list to store the start times of each bin (interval)
time_bins = list()
# Create an empty list to store the midpoints of each bin (for plotting and correlation purposes)
bin_midpoints = list()
# Create an empty list to store the average noise values for each bin
noise_averages = list()
# Create an empty list to store the count of data points in each bin
bin_counts = list()
# Initialize the index to track the bin being processed and store results in the lists
i = 1
# Begin a loop that iterates through the proper time scale, excluding the last value
# The last value is excluded because it serves as the end time of the final bin
for (bin_start in proper_time_scale[-length(proper_time_scale)]) {
# Define the end time of the current bin by adding the interval size to 'bin_start'
bin_end = bin_start + interval
# Calculate the midpoint of the current bin as the average of 'bin_start' and 'bin_end'
bin_mid = bin_start + (bin_end - bin_start) / 2
# Filter the dataset to include only rows with timestamps within the current bin
bin_data = open_data[open_data$Datetime >= bin_start & open_data$Datetime < bin_end, ]
# Count the number of rows in the filtered dataset to determine how many data points are in the bin
count = nrow(bin_data)
# Check if the current bin contains data points
if (count > 0) {
# If data points exist, calculate the average noise value for the bin
noise_avg = sum(bin_data$Noise) / count
} else {
# If no data points exist, assign NA as the average noise value for the bin
noise_avg = NA
}
# Add the start time of the current bin to the 'time_bins' list as a string
time_bins[[i]] = as.character(bin_start)
# Add the midpoint time of the current bin to the 'bin_midpoints' list as a string
bin_midpoints[[i]] = as.character(bin_mid)
# Add the calculated average noise value to the 'noise_averages' list
noise_averages[[i]] = noise_avg
# Add the count of data points in the bin to the 'bin_counts' list
bin_counts[[i]] = count
# Increment the index to track the next bin
i = i + 1
}
# Combine the lists into a data frame for easy analysis and visualization
result_open = data.frame(
TimeBin = unlist(time_bins),        # Convert the 'time_bins' list into a column
BinMidpoint = unlist(bin_midpoints),# Convert the 'bin_midpoints' list into a column
Noise_avg = unlist(noise_averages), # Convert the 'noise_averages' list into a column
Count = unlist(bin_counts)          # Convert the 'bin_counts' list into a column
)
# Print the resulting data frame to view the processed bins and their statistics
print(result_open)
```{r small hole vs big hole, include=FALSE}
# plot 3 small hole vs big hole
library(ggplot2)
# Load dplyr to use the pipe operator (%>%)
library(dplyr)
# Process result_small_hole datafram
# Remove rows with NA in the Noise_avg column
result_small_hole = result_small_hole %>% filter(!is.na(Noise_avg))
# Process result_big_hole dataframe
# Remove rows with NA in the Noise_avg column
result_big_hole = result_big_hole %>% filter(!is.na(Noise_avg))
# Merge the two dataframes on the 'BinMidpoint' column
merged_data = merge(
result_small_hole,       # First dataframe
result_big_hole,         # Second dataframe
by = "BinMidpoint",      # Column to merge on
suffixes = c("_small_hole", "_big_hole")  # Add suffixes to distinguish similar column names
)
# Fit a linear model to get the slope, intercept, and R-squared value
lm_model = lm(Noise_avg_big_hole ~ Noise_avg_small_hole, data = merged_data)
# Extract the slope (second coefficient), intercept (first coefficient), and R-squared value, and the Pearson correlation coefficient
slope = coef(lm_model)[2]  # The slope corresponds to Noise_avg_small_hole
intercept = coef(lm_model)[1]  # The intercept is the first coefficient
r_squared = summary(lm_model)$r.squared  # R-squared value
# Calculate the Pearson correlation coefficient
correlation = cor(merged_data$Noise_avg_small_hole, merged_data$Noise_avg_big_hole)
# Create the plot
ggplot(data = merged_data, aes(x = Noise_avg_small_hole, y = Noise_avg_big_hole)) +
geom_point(color = "blue", size = 2) +  # Scatter points
geom_smooth(method = "lm", color = "red", se = FALSE) +  # Linear regression line
labs(
title = "Noise Averages: Small Hole vs Big Hole",  # Title of the plot
x = "Small Hole Noise Average dB",                 # Label for x-axis
y = "Big Hole Noise Average dB"                    # Label for y-axis
) +
theme_minimal() +  # Clean theme
# Add slope, intercept, R², and correlation coefficient as text annotation
annotate("text",
x = max(merged_data$Noise_avg_small_hole) * 0.8,
y = max(merged_data$Noise_avg_big_hole) * 0.9,
label = paste("Slope:", round(slope, 2),
"\nIntercept:", round(intercept, 2),
"\nR²:", round(r_squared, 2),
"\nCorrelation:", round(correlation, 2)),  # Added correlation to the label
color = "black", size = 5, hjust = 0)  # Position and style of the text
